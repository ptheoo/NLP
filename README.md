# NLP â€” BÃ¡o cÃ¡o (Lab 1 & Lab 2)

> ÄÃ¢y lÃ  bÃ¡o cÃ¡o tÃ³m táº¯t quÃ¡ trÃ¬nh Ä‘Ã£ thá»±c hiá»‡n trong **Lab 1** vÃ  **Lab 2**: xÃ¢y dá»±ng tokenizer, loader cho dataset UD_English-EWT, triá»ƒn khai CountVectorizer, vÃ  cháº¡y thá»­ nghiá»‡m.

---

## 1. Má»¥c tiÃªu

### Lab 1
- Táº¡o interface `Tokenizer` (chuáº©n hÃ³a thiáº¿t káº¿).
- CÃ i Ä‘áº·t:
  - `SimpleTokenizer`: tÃ¡ch token theo khoáº£ng tráº¯ng.
  - `RegexTokenizer`: tÃ¡ch token báº±ng regex `\w+|[^\w\s]`.
- Viáº¿t `load_raw_text_data` Ä‘á»ƒ load vÄƒn báº£n tá»« dataset UD (CoNLL-U).
- Viáº¿t script test cho tokenizer trÃªn cÃ¢u máº«u + dá»¯ liá»‡u tháº­t.

### Lab 2
- Táº¡o interface `Vectorizer`.
- CÃ i Ä‘áº·t `CountVectorizer`:
  - `fit(corpus)`: sinh vocabulary.
  - `transform(corpus)`: táº¡o document-term matrix.
- Test trÃªn corpus nhá» + má»™t sample tá»« UD dataset.

---

## 2. Cáº¥u trÃºc thÆ° má»¥c
```
NLP/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ interfaces.py # Interface: Tokenizer, Vectorizer
| | â””â”€â”€dataset_loader.py
â”‚ â”œâ”€â”€ preprocessing/
â”‚ â”‚ â”œâ”€â”€ simple_tokenizer.py # Tokenizer Ä‘Æ¡n giáº£n (split)
â”‚ â”‚ â””â”€â”€ regex_tokenizer.py # Tokenizer dÃ¹ng regex
â”‚ â”œâ”€â”€ core/dataset_loaders.py # HÃ m load dá»¯ liá»‡u tá»« UD .conllu
â”‚ â””â”€â”€ representations/count_vectorizer.py # CountVectorizer
â”œâ”€â”€ test/
â”‚ â”œâ”€â”€ lab1_test1.py # Test SimpleTokenizer cÆ¡ báº£n
â”‚ â”œâ”€â”€ lab1_test2.py # So sÃ¡nh SimpleTokenizer vs RegexTokenizer
â”‚ â”œâ”€â”€ lab1_test3.py # Test tokenizer trÃªn UD dataset
â”‚ â””â”€â”€ lab2_test.py # Test CountVectorizer
â”œâ”€â”€ UD_English-EWT/ # Dá»¯ liá»‡u (CoNLL-U)
â”‚ â””â”€â”€ en_ewt-ud-train.conllu
â””â”€â”€ README.md
```

---

## 3. Chuáº©n bá»‹ mÃ´i trÆ°á»ng

Táº¡o vÃ  kÃ­ch hoáº¡t mÃ´i trÆ°á»ng áº£o (Windows):

```bash
python -m venv venv
venv\Scripts\activate

```
CÃ i dependencies:
```bash
pip install -r requirements.txt
```
## 4. CÃ¡ch cháº¡y

Cháº¡y test cho tá»«ng pháº§n:
```bash
python -m test.lab1_test1
python -m test.lab1_test2
python -m test.lab1_test3
python -m test.lab2_test
```
## 5. Káº¿t quáº£ & PhÃ¢n tÃ­ch
### Lab 1. Task 1 â€” SimpleTokenizer cÆ¡ báº£n
```bash
python -m test.lab1_test1
```
['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']


ğŸ‘‰ Tokenizer Ä‘Ã£ tÃ¡ch tá»« vÃ  dáº¥u cÃ¢u riÃªng biá»‡t. 

### Lab 1. Task 2 â€” So sÃ¡nh SimpleTokenizer vs RegexTokenizer
```
Sentence: Hello, world! This is a test.
SimpleTokenizer: ['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']
RegexTokenizer:   ['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']

Sentence: NLP is fascinating... isn't it?
SimpleTokenizer: ['nlp', 'is', 'fascinating', '.', '.', '.', "isn't", 'it', '?']
RegexTokenizer:   ['nlp', 'is', 'fascinating', '.', '.', '.', 'isn', "'", 't', 'it', '?']

Sentence: Let's see how it handles 123 numbers and punctuation!
SimpleTokenizer: ["let's", 'see', 'how', 'it', 'handles', '123', 'numbers', 'and', 'punctuation', '!']
RegexTokenizer:   ['let', "'", 's', 'see', 'how', 'it', 'handles', '123', 'numbers', 'and', 'punctuation', '!']
```

ğŸ‘‰ Nháº­n xÃ©t:

Vá»›i cÃ¢u Ä‘Æ¡n giáº£n, 2 tokenizer cho káº¿t quáº£ giá»‘ng nhau.

Vá»›i cÃ¢u chá»©a dáº¥u nhÃ¡y ('), RegexTokenizer chi tiáº¿t hÆ¡n: "isn't" â†’ ['isn', "'", 't'], "let's" â†’ ['let', "'", 's'].

Äiá»u nÃ y phÃ¹ há»£p cho xá»­ lÃ½ NLP chuyÃªn sÃ¢u (POS tagging, lemmatization).

### Lab 1. Task 3 â€” UD Dataset Sample
```
--- Tokenizing Sample Text from UD_English-EWT ---
Original Sample: From the AP comes this story : President Bush ...
SimpleTokenizer Output (first 20 tokens): ['from', 'the', 'ap', 'comes', 'this', 'story', ':', 'president', 'bush', 'on', 'tuesday', 'nominated', 'two', 'individuals', 'to', 'replace', 'retiring', 'jurists', 'on', 'federal']
RegexTokenizer Output (first 20 tokens):  ['from', 'the', 'ap', 'comes', 'this', 'story', ':', 'president', 'bush', 'on', 'tuesday', 'nominated', 'two', 'individuals', 'to', 'replace', 'retiring', 'jurists', 'on', 'federal']
```

ğŸ‘‰ Nháº­n xÃ©t:

TrÃªn dá»¯ liá»‡u thá»±c, 2 tokenizer cho káº¿t quáº£ tÆ°Æ¡ng Ä‘á»“ng á»Ÿ 20 token Ä‘áº§u.

RegexTokenizer váº«n cÃ³ lá»£i tháº¿ á»Ÿ cÃ¡c cÃ¢u chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t phá»©c táº¡p.

### Lab 2 â€” CountVectorizer
```
Corpus:

I love NLP.
I love programming.
NLP is a subfield of AI.


Vocabulary:

.: 0
a: 1
ai: 2
i: 3
is: 4
love: 5
nlp: 6
of: 7
programming: 8
subfield: 9


Document-Term Matrix:

[1, 0, 0, 1, 0, 1, 1, 0, 0, 0]
[1, 0, 0, 1, 0, 1, 0, 0, 1, 0]
[1, 1, 1, 0, 1, 0, 1, 1, 0, 1]
```

ğŸ‘‰ Nháº­n xÃ©t:

Vector biá»ƒu diá»…n táº§n suáº¥t token.

VÃ­ dá»¥: cÃ¢u I love NLP. cÃ³ i=1, love=1, nlp=1.

UD Dataset Sample (rÃºt gá»n):
```
Vocabulary (trÃ­ch):

*: 0
,: 1
-: 2
.: 3
15: 4
...
washington: 70
wheel: 71
with: 72
year: 73
years: 74


Document-Term Matrix (3 dÃ²ng Ä‘áº§u, rÃºt gá»n):

[0, 0, 1, 2, 1, 0, 1, 1, ...]
[3, 1, 1, 3, 1, 0, 0, 2, ...]
[0, 1, 3, 2, 0, 1, 0, 1, ...]

```
ğŸ‘‰ Nháº­n xÃ©t:

Vocabulary lá»›n, pháº£n Ã¡nh tÃ­nh Ä‘a dáº¡ng cá»§a dá»¯ liá»‡u tháº­t.

Ma tráº­n sparse, Ä‘áº·c trÆ°ng cho vÄƒn báº£n lá»›n.

CountVectorizer Ä‘Ã£ biá»ƒu diá»…n thÃ nh cÃ´ng corpus thÃ nh vector space.

## 6. Káº¿t luáº­n

Lab 1: ÄÃ£ xÃ¢y dá»±ng 2 tokenizer hoáº¡t Ä‘á»™ng á»•n, RegexTokenizer chi tiáº¿t hÆ¡n.

Lab 2: CountVectorizer triá»ƒn khai thÃ nh cÃ´ng, Ã¡p dá»¥ng tá»‘t cáº£ trÃªn corpus nhá» vÃ  dataset tháº­t.

Há»‡ thá»‘ng hiá»‡n Ä‘Ã£ cÃ³ pipeline cÆ¡ báº£n: tokenizer â†’ vectorizer â†’ matrix representation.

Tiáº¿p theo cÃ³ thá»ƒ má»Ÿ rá»™ng vá»›i:

TF-IDF Vectorizer.

Loáº¡i bá» stopword.

Word embeddings (Word2Vec, BERT).