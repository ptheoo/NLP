# Lab3 ‚Äî Word Embeddings (Lab Report)

M·ª•c ti√™u: Th·ª±c hi·ªán c√°c thao t√°c v·ªõi word embeddings: s·ª≠ d·ª•ng pretrained embedding (GloVe), hu·∫•n luy·ªán Word2Vec t·ª´ d·ªØ li·ªáu nh·ªè (Gensim) v√† l·ªõn (Spark), t·∫°o document embedding, gi·∫£m chi·ªÅu & tr·ª±c quan h√≥a, v√† ph√¢n t√≠ch k·∫øt qu·∫£.


## M·ª•c l·ª•c

1. T√≥m t·∫Øt ti·∫øn ƒë·ªô (Checklist)

2. M√¥i tr∆∞·ªùng & C√†i ƒë·∫∑t

3. H∆∞·ªõng d·∫´n ch·∫°y (How to run)

4. N·ªôi dung th·ª±c thi & k·∫øt qu·∫£ ch√≠nh (Outputs)

4.1 Spark Word2Vec demo (k·∫øt qu·∫£)

4.2 S·ª≠ d·ª•ng GloVe pretrained (lab4_test)

4.3 Hu·∫•n luy·ªán Word2Vec t·ª´ ƒë·∫ßu (lab4_embedding_training_demo)

5. Ph√¢n t√≠ch & Nh·∫≠n x√©t chi ti·∫øt (Ph·∫ßn Quan tr·ªçng)

5.1 Pretrained GloVe ‚Äî ch·∫•t l∆∞·ª£ng & nh·∫≠n x√©t

5.2 Word2Vec t·ª± hu·∫•n luy·ªán tr√™n en_ewt ‚Äî v√¨ sao k·∫øt qu·∫£ ‚Äúk·ª≥ l·∫°‚Äù?

5.3 So s√°nh: Pretrained vs Trained-from-scratch

5.4 Gi·∫£m chi·ªÅu & tr·ª±c quan h√≥a 

6. C√°c v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i & c√°ch gi·∫£i quy·∫øt (Troubleshooting)

7. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn & b∆∞·ªõc ti·∫øp theo


## 1. T√≥m t·∫Øt ti·∫øn ƒë·ªô (Checklist)

Ph·∫ßn 1: Tri·ªÉn khai (50%)

 Task 1: T·∫£i v√† s·ª≠ d·ª•ng pretrained model (Gensim) ‚Äî glove-wiki-gigaword-50 (ƒë√£ t·∫£i th√†nh c√¥ng).

 L·∫•y vector c·ªßa m·ªôt t·ª´ (king) ‚Äî OK.

 T√≠nh similarity (king vs queen, king vs man) ‚Äî OK.

 T√¨m c√°c t·ª´ most_similar (computer) ‚Äî OK.

 Task 2: Nh√∫ng c√¢u/vƒÉn b·∫£n b·∫±ng trung b√¨nh vector ‚Äî OK.

 Task 3: Hu·∫•n luy·ªán model Word2Vec t·ª´ d·ªØ li·ªáu th√¥ (en_ewt-ud-train.txt) ‚Äî ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng, model l∆∞u ƒë∆∞·ª£c.

 L∆∞u & t·∫£i l·∫°i model hu·∫•n luy·ªán ‚Äî OK.

 Task 4: Hu·∫•n luy·ªán model tr√™n t·∫≠p d·ªØ li·ªáu l·ªõn (Spark) ‚Äî ƒê√£ ch·∫°y demo Spark Word2Vec (k·∫øt qu·∫£ hi·ªÉn th·ªã).

 Task 5: Tr·ª±c quan h√≥a embedding b·∫±ng PCA/t-SNE ‚Äî Ho√†n th√†nh

Ph·∫ßn 2: B√°o c√°o v√† Ph√¢n t√≠ch (50%)

 Gi·∫£i th√≠ch c√°c b∆∞·ªõc th·ª±c hi·ªán ‚Äî c√≥ trong b√°o c√°o n√†y.

 H∆∞·ªõng d·∫´n ch·∫°y code ‚Äî c√≥.

 Ph√¢n t√≠ch k·∫øt qu·∫£ ‚Äî c√≥ (m·ª•c 5).

 Nh·∫≠n x√©t v·ªÅ similarity / most_similar (pretrained) ‚Äî c√≥.

 Ph√¢n t√≠ch tr·ª±c quan h√≥a ‚Äî c√≥ (Trong file PDF)

 So s√°nh pretrained vs t·ª± hu·∫•n luy·ªán ‚Äî c√≥.

 Kh√≥ khƒÉn & gi·∫£i ph√°p ‚Äî c√≥.


## 2. M√¥i tr∆∞·ªùng & C√†i ƒë·∫∑t

Khuy·∫øn ngh·ªã m√¥i tr∆∞·ªùng (venv):

Python 3.10

Virtual environment (v√≠ d·ª• venv) ‚Äî b·∫°n ƒëang d√πng (venv).

Th√™m v√†o ```requirements.txt``` c√°c th∆∞ vi·ªán sau:

```bash
gensim
nltk
numpy==1.26.4
scipy==1.11.4
matplotlib
scikit-learn
pyspark
```

L∆∞u √Ω: ƒë√£ t·ª´ng g·∫∑p xung ƒë·ªôt numpy vs thinc ‚Äî trong lab n√†y phi√™n b·∫£n numpy==1.26.4 + scipy==1.11.4 ho·∫°t ƒë·ªông ·ªïn v·ªõi gensim. N·∫øu b·∫°n c√†i spacy/thinc c√≥ th·ªÉ xu·∫•t c·∫£nh b√°o t∆∞∆°ng th√≠ch, kh√¥ng g√¢y l·ªói cho c√°c t√°c v·ª• hi·ªán t·∫°i.

NLTK: c·∫ßn download punkt ƒë·ªÉ tokenize:
```bash
import nltk
nltk.download('punkt')
```
## 3. H∆∞·ªõng d·∫´n ch·∫°y (How to run)

K√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o:

Windows PowerShell
```bash
.\venv\Scripts\Activate.ps1
```
ho·∫∑c cmd:
```bash
.\venv\Scripts\activate.bat
```

C√†i ƒë·∫∑t dependencies:
```bash
pip install -r requirements.txt
```

Ch·∫°y c√°c script:

Spark Word2Vec demo:
```bash
python test/lab4_spark_word2vec_demo.py
```

S·ª≠ d·ª•ng pretrained GloVe v√† test c√°c h√†m:
```bash
python test/lab4_test.py
```

Hu·∫•n luy·ªán Word2Vec t·ª´ en_ewt:
```bash
python test/lab4_embedding_training_demo.py
```

## 4. N·ªôi dung th·ª±c thi & k·∫øt qu·∫£ ch√≠nh (Outputs)

D∆∞·ªõi ƒë√¢y l√† c√°c output ƒë√£ ch·∫°y:
### 4.1 S·ª≠ d·ª•ng GloVe pretrained (lab4_test)
```bash
[nltk_data] Downloading package punkt...
üîπ ƒêang t·∫£i m√¥ h√¨nh 'glove-wiki-gigaword-50' ...
 M√¥ h√¨nh 'glove-wiki-gigaword-50' t·∫£i th√†nh c√¥ng (50-dim).

--- üîπ L·∫•y vector c·ªßa t·ª´ 'king' ---
K√≠ch th∆∞·ªõc vector: (50,)
Gi√° tr·ªã ƒë·∫ßu ti√™n: [ 0.50451   0.68607 -0.59517 -0.022801  0.60046 ]        

--- üîπ ƒê·ªô t∆∞∆°ng ƒë·ªìng ---
king vs queen: 0.78390425
king vs man: 0.53093773

--- üîπ 10 t·ª´ g·∫ßn nghƒ©a v·ªõi 'computer' ---
computers       -> 0.9165
software        -> 0.8815
technology      -> 0.8526
electronic      -> 0.8126
internet        -> 0.8060
computing       -> 0.8026
devices         -> 0.8016
digital         -> 0.7992
applications    -> 0.7913
pc              -> 0.7883

--- üîπ Vector vƒÉn b·∫£n ---
Vector bi·ªÉu di·ªÖn vƒÉn b·∫£n:
[ 0.04564168  0.36530998 -0.55974334  0.04014383  0.09655549  0.15623933
 -0.33622834 -0.12495166 -0.01031508 -0.5006717 ]
ƒê·ªô d√†i vector: 50
```
### 4.2 Hu·∫•n luy·ªán Word2Vec t·ª´ ƒë·∫ßu (lab4_embedding_training_demo) ‚Äî k·∫øt qu·∫£ sample
```bash
B·∫ÆT ƒê·∫¶U: HU·∫§N LUY·ªÜN M√î H√åNH WORD2VEC T·ª™ ƒê·∫¶U
...
T·ªïng s·ªë c√¢u ƒë∆∞·ª£c ƒë·ªçc ƒë·ªÉ hu·∫•n luy·ªán: 14225
...
Word2Vec lifecycle event {... vocab=3866, vector_size=100 ...}
Hu·∫•n luy·ªán m√¥ h√¨nh Word2Vec ho√†n t·∫•t.
K√≠ch th∆∞·ªõc t·ª´ v·ª±ng m√¥ h√¨nh (vocab size): 3866

ƒêang l∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán t·∫°i: .../results/word2vec_ewt.model
L∆∞u m√¥ h√¨nh th√†nh c√¥ng.

Demo s·ª≠ d·ª•ng m√¥ h√¨nh Word2Vec ƒë√£ hu·∫•n luy·ªán:

   A. 10 t·ª´ t∆∞∆°ng ƒë·ªìng nh·∫•t v·ªõi 'student':
      1. science: 0.4967
      2. canada,: 0.4903
      3. buy: 0.4637
      4. core: 0.4627
      5. brief: 0.4625
      6. unlimited: 0.4568
      7. parent: 0.4512
      8. reports,: 0.4425
      9. visa: 0.4393
      10. agel: 0.4387

      ...
   B. Gi·∫£i quy·∫øt b√†i to√°n t∆∞∆°ng t·ª±: king - man + woman = ?
      K·∫øt qu·∫£ (Top 3):
      1. arabia (Score: 0.4022)
      2. foot (Score: 0.3916)
      3. "it (Score: 0.3914)

```
Ghi ch√∫: k·∫øt qu·∫£ analogies sai l√† do h·∫°n ch·∫ø t·∫≠p hu·∫•n luy·ªán (xem ph·∫ßn ph√¢n t√≠ch).

### 4.3 Spark Word2Vec demo (k·∫øt qu·∫£)
```bash
Kh·ªüi t·∫°o SparkSession.
...
----------
ƒê·ªçc d·ªØ li·ªáu
S·ªë d√≤ng ƒë·ªçc ƒë∆∞·ª£c: 30000
----------
Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n v√† Tokenization
S·ªë d√≤ng sau khi l·ªçc c√°c d√≤ng tr·ªëng: 30000
DataFrame sau khi Tokenization:
+--------------------------------------------------+
|                   words                          |
+--------------------------------------------------+
|[beginners, bbq, class, taking, place, in, miss...]|
...
only showing top 5 rows
----------
Hu·∫•n luy·ªán m√¥ h√¨nh Word2Vec (Skip-gram)
25/10/15 00:29:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS

T√¨m c√°c t·ª´ t∆∞∆°ng t·ª± 'computer'
+----------+------------------+
|word      |similarity        |
+----------+------------------+
|desktop   |0.6939913630485535|
|computers |0.6567586064338684|
|software  |0.6353139281272888|
|coding    |0.6132869720458984|
|interfaces|0.6102142333984375|
|laptop    |0.6024858951568604|
|robohelp  |0.5980682373046875|
|backup    |0.5902743339538574|
|pc        |0.5899812579154968|
|graphical |0.5812470316886902|
+----------+------------------+

Ho√†n th√†nh hu·∫•n luy·ªán Spark Word2Vec
...

```

## 5. Ph√¢n t√≠ch & Nh·∫≠n x√©t chi ti·∫øt (Ph·∫ßn Quan tr·ªçng)
### 5.1 Pretrained GloVe ‚Äî ch·∫•t l∆∞·ª£ng & nh·∫≠n x√©t

glove-wiki-gigaword-50 l√† embedding ti·ªÅn hu·∫•n luy·ªán tr√™n corpora l·ªõn (Wikipedia + Gigaword).

K·∫øt qu·∫£ king vs queen ‚âà 0.78 v√† most_similar cho computer ƒë·ªÅu r·∫•t h·ª£p l√Ω ‚Äî cho th·∫•y pretrained embeddings mang ki·∫øn th·ª©c ng·ªØ nghƒ©a s√¢u r·ªông.

∆Øu ƒëi·ªÉm: kh√¥ng c·∫ßn hu·∫•n luy·ªán, ·ªïn ƒë·ªãnh, r·∫•t ph√π h·ª£p cho b√†i t·∫≠p demo / baseline.

Nh∆∞·ª£c ƒëi·ªÉm: kh√¥ng domain-specific; n·∫øu d·ªØ li·ªáu c·ªßa b·∫°n kh√°c bi·ªát (v√≠ d·ª• vƒÉn b·∫£n y t·∫ø/t√†i ch√≠nh), pretrained c√≥ th·ªÉ kh√¥ng ph·∫£n √°nh t·ªët thu·∫≠t ng·ªØ chuy√™n ng√†nh.

### 5.2 Word2Vec t·ª± hu·∫•n luy·ªán tr√™n en_ewt

Quan s√°t: ph√©p analogies king - man + woman cho k·∫øt qu·∫£ nh∆∞ arabia, foot, "it ho·∫∑c easily (trong m·ªôt l·∫ßn kh√°c) ‚Äî kh√¥ng ph·∫£i queen. Nguy√™n nh√¢n ch√≠nh:

T·∫≠p d·ªØ li·ªáu nh·ªè & h·∫°n ch·∫ø (14225 c√¢u, ~177k t·ª´):

M·ªôt s·ªë t·ª´ quan tr·ªçng (king/queen) c√≥ t·∫ßn su·∫•t r·∫•t th·∫•p ‚Üí embedding b·ªã noisy.

Vocab b·ªã c·∫Øt (effective_min_count=5):

Gensim ƒë√£ lo·∫°i b·ªè t·ª´ √≠t xu·∫•t hi·ªán, l√†m m·∫•t c√°c t·ª´ c·∫ßn cho analogies.

D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng/ng·ªØ c·∫£nh ngh√®o:

M·ªëi quan h·ªá king ‚Üî queen c·∫ßn nhi·ªÅu ng·ªØ c·∫£nh so s√°nh (royalty contexts). N·∫øu kh√¥ng ƒë·ªß, m√¥ h√¨nh kh√¥ng h·ªçc ƒë∆∞·ª£c.

Thu·∫≠t to√°n & si√™u tham s·ªë: epochs, window, vector_size ·∫£nh h∆∞·ªüng m·∫°nh. D√π ƒë√£ l·∫∑p nhi·ªÅu epoch (·ªü output b·∫°n ƒë√£ ch·∫°y nhi·ªÅu epoch), n·∫øu d·ªØ li·ªáu thi·∫øu bi·ªÉu di·ªÖn ng·ªØ c·∫£nh th√¨ v·∫´n kh√¥ng t·ªët.

H·ªá qu·∫£: m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá c·ª•c b·ªô/ƒë·ªìng xu·∫•t hi·ªán (co-occurrence) ch·ª© ch∆∞a h·ªçc ƒë∆∞·ª£c quy lu·∫≠t ng·ªØ nghƒ©a s√¢u.

### 5.3 So s√°nh: Pretrained vs Trained-from-scratch
```
Ti√™u ch√≠	        |  Pretrained (GloVe)	     |  Trained-from-scratch (EWT)
____________________|____________________________|_______________________
D·ªØ li·ªáu hu·∫•n luy·ªán	|   R·∫•t l·ªõn	                 |   Nh·ªè (~17k c√¢u)
Ch·∫•t l∆∞·ª£ng analogies|	T·ªët (king‚Üíqueen)	     |  K√©m / noisy
Ph√π h·ª£p domain	    |   Chung chung	             |C√≥ th·ªÉ domain-specific (n·∫øu corpus domain-specific)
Th·ªùi gian	        |  T·∫£i nhanh, kh√¥ng c·∫ßn train|	C·∫ßn th·ªùi gian train
Khi n√†o d√πng	    |   Baseline, nhanh	         |   Khi c·∫ßn embedding chuy√™n ng√†nh
```

K·∫øt lu·∫≠n: V·ªõi d·ªØ li·ªáu nh·ªè, d√πng pretrained ƒë·ªÉ l√†m baseline; t·ª± hu·∫•n luy·ªán ch·ªâ th·ª±c s·ª± hi·ªáu qu·∫£ n·∫øu c√≥ corpus ƒë·ªß l·ªõn ho·∫∑c domain-specific.

### 5.4 Gi·∫£m chi·ªÅu & tr·ª±c quan h√≥a

PCA: nhanh, tuy·∫øn t√≠nh ‚Äî d√πng ƒë·ªÉ c√≥ c√°i nh√¨n t·ªïng quan.

t-SNE / UMAP: t√°ch c·ª•m t·ªët h∆°n, ph√π h·ª£p cho visual analysis.

Nh·∫≠n x√©t, ƒë√°nh gi√° c·ª• th·ªÉ h∆°n trong  file PDF ```22001286_NguyenThiPhuongThao_Lab3_Phan1```.



## 6. C√°c v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i & c√°ch gi·∫£i quy·∫øt (Troubleshooting)
A. ImportError do SciPy / NumPy

L·ªói: ImportError: cannot import name 'triu' from 'scipy.linalg'

Gi·∫£i ph√°p: h·∫° scipy v·ªÅ 1.11.4 v√† d√πng numpy==1.26.4. L∆∞u √Ω xung ƒë·ªôt v·ªõi thinc/spacy ‚Äî n·∫øu kh√¥ng d√πng spacy, c√≥ th·ªÉ g·ª° thinc/spacy ho·∫∑c b·ªè qua c·∫£nh b√°o.

B. ModuleNotFoundError: No module named 'src'

Nguy√™n nh√¢n: ch·∫°y script t·ª´ th∆∞ m·ª•c con.

Gi·∫£i ph√°p:

Ch·∫°y b·∫±ng: python -m test.lab4_test t·ª´ project root.

Ho·∫∑c th√™m sys.path.append(...) trong script test ƒë·ªÉ th√™m th∆∞ m·ª•c g·ªëc v√†o sys.path.

Ho·∫∑c t·∫°o __init__.py ph√π h·ª£p cho package.

C. t-SNE AttributeError: 'list' object has no attribute 'shape'

Gi·∫£i ph√°p: convert vectors_np = np.array(vectors) tr∆∞·ªõc khi g·ªçi tsne.fit_transform(vectors_np).

D. K·∫øt qu·∫£ analogiesÂùè (king ‚Üí easily)

Nguy√™n nh√¢n: d·ªØ li·ªáu nh·ªè / min_count lo·∫°i b·ªè t·ª´ / thi·∫øu ng·ªØ c·∫£nh.

Gi·∫£i ph√°p: d√πng corpus l·ªõn h∆°n (text8, wikipedia) ho·∫∑c gi·∫£m min_count, tƒÉng epochs, tƒÉng window, ho·∫∑c d√πng pretrained.

## 7. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn & b∆∞·ªõc ti·∫øp theo


So s√°nh k·ªπ h∆°n pretrained vs trained-from-scratch:

L·∫•y m·ªôt s·ªë c·∫∑p test (king-queen, paris-france, doctor-nurse) v√† ƒëo cosine similarities tr√™n c·∫£ hai model. T·ªïng h·ª£p v√†o b·∫£ng.

Th·ª≠ FastText: t·ªët v·ªõi t·ª´ hi·∫øm v√† OOV (subword).

TƒÉng/ƒë·ªïi tham s·ªë hu·∫•n luy·ªán: gi·∫£m min_count, tƒÉng epochs (n·∫øu d·ªØ li·ªáu ƒë·ªß), th·ª≠ window l·ªõn h∆°n.

D√πng corpus l·ªõn h∆°n: text8 (s·∫µn c√≥), Wikipedia (t·∫£i via gensim.downloader) ‚Üí hu·∫•n luy·ªán s·∫Ω cho analogies t·ªët.

ƒê·ªãnh l∆∞·ª£ng: ngo√†i tr·ª±c quan, th·ª±c hi·ªán ƒë√°nh gi√° ƒë·ªãnh l∆∞·ª£ng nh∆∞ intrinsic evaluation (word similarity datasets) n·∫øu c·∫ßn.