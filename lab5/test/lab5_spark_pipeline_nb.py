# NLP/lab5/test/spark_sentiment_nb.py
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


def run_spark_pipeline_nb():
    """
    PhÃ¢n tÃ­ch cáº£m xÃºc sá»­ dá»¥ng PySpark ML Pipeline vá»›i Naive Bayes.
    ÄÃ¡nh giÃ¡ báº±ng Accuracy, Precision, Recall, F1-score.
    """

    # --- 1. Khá»Ÿi táº¡o Spark ---
    print("=== 1. Khá»Ÿi táº¡o Spark Session ===")
    spark = SparkSession.builder.appName("SentimentAnalysis_NaiveBayes").getOrCreate()
    print("Spark Session khá»Ÿi táº¡o thÃ nh cÃ´ng.")

    # --- 2. Äá»c dá»¯ liá»‡u ---
    print("\n=== 2. Táº£i vÃ  xá»­ lÃ½ dá»¯ liá»‡u ===")
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.abspath(os.path.join(current_dir, '..', '..', 'data', 'sentiments.csv'))
    data_path_uri = "file:///" + data_path.replace("\\", "/")

    if not os.path.exists(data_path):
        print(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u táº¡i: {data_path}")
        spark.stop()
        return

    # Äá»c CSV
    df = spark.read.csv(data_path_uri, header=True, inferSchema=True)

    # Chuyá»ƒn sentiment (-1, 1) -> label (0, 1)
    df = df.withColumn("label", (col("sentiment").cast("integer") + lit(1)) / lit(2))

    # Loáº¡i bá» dÃ²ng null
    df = df.dropna(subset=["sentiment", "text"])
    total = df.count()
    print(f"Sá»‘ dÃ²ng dá»¯ liá»‡u sau khi lÃ m sáº¡ch: {total}")

    # Chia táº­p huáº¥n luyá»‡n / kiá»ƒm thá»­
    train, test = df.randomSplit([0.8, 0.2], seed=42)
    print(f"Train: {train.count()} | ğŸ§ª Test: {test.count()}")

    # --- 3. XÃ¢y dá»±ng pipeline ---
    print("\n=== 3. XÃ¢y dá»±ng pipeline Naive Bayes ===")
    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
    hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=5000)
    idf = IDF(inputCol="raw_features", outputCol="features")
    nb = NaiveBayes(featuresCol="features", labelCol="label", smoothing=1.0, modelType="multinomial")

    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, nb])

    # --- 4. Huáº¥n luyá»‡n ---
    print("\n=== 4. Huáº¥n luyá»‡n mÃ´ hÃ¬nh ===")
    model = pipeline.fit(train)
    print("MÃ´ hÃ¬nh Naive Bayes huáº¥n luyá»‡n xong.")

    # --- 5. Dá»± Ä‘oÃ¡n & Ä‘Ã¡nh giÃ¡ ---
    print("\n=== 5. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh ===")
    preds = model.transform(test)

    preds.select("label", "prediction", "text").show(5, truncate=40)

    # 5a. Accuracy & F1-score
    evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction")
    accuracy = evaluator.evaluate(preds, {evaluator.metricName: "accuracy"})
    f1_score = evaluator.evaluate(preds, {evaluator.metricName: "f1"})

    # 5b. Precision & Recall (tá»± tÃ­nh)
    TP = preds.filter("label = 1.0 AND prediction = 1.0").count()
    FP = preds.filter("label = 0.0 AND prediction = 1.0").count()
    FN = preds.filter("label = 1.0 AND prediction = 0.0").count()

    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0

    # --- 6. In káº¿t quáº£ ---
    print("\nKáº¾T QUáº¢ ÄÃNH GIÃ MÃ” HÃŒNH (Naive Bayes)")
    print("-" * 50)
    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-score : {f1_score:.4f}")
    print("-" * 50)

    # --- 7. Dá»«ng Spark ---
    spark.stop()
    print("\nÄÃ£ dá»«ng Spark Session.")


if __name__ == "__main__":
    run_spark_pipeline_nb()
